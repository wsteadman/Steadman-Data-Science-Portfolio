{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNuiLhwPXT2/9GOv6o92FK1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Supervised Machine Learning: Linear Regression"],"metadata":{"id":"tSYyrxq5csQJ"}},{"cell_type":"markdown","source":["## Linear Regression: Unscaled vs. Scaled Data\n","In this demo, we follow the ML process:\n","1. **Remember:** Load and inspect the data.\n","2. **Formulate:** Build a linear regression model first on raw (unscaled) data.\n","3. **Predict:** Evaluate the model's performance.\n","\n","Then we apply feature scaling and rebuild the model to compare results.\n","We use the Student Performance dataset from Kaggle to predict the \"Performance Index\" of students."],"metadata":{"id":"SvloYQVhZzGh"}},{"cell_type":"code","source":["# import neccesary libraries\n","import pandas as pd\n","import numpy as np\n","\n","# Download data from Kaggle\n","!kaggle datasets download -d nikhil7280/student-performance-multiple-linear-regression\n","!unzip student-performance-multiple-linear-regression.zip\n","\n","# Import dataframe\n","df = pd.read_csv(\"Student_Performance.csv\")\n","df"],"metadata":{"id":"KR7yBXm0Hltx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert extracurricular activities to numeric\n","df[\"Extracurricular Activities\"] = df[\"Extracurricular Activities\"].map({\"Yes\":1, \"No\":0})\n","\n","# Define the features and target variable based on the dataset\n","feature_vars = [\"Hours Studied\", \"Previous Scores\", \"Sleep Hours\",\n","                \"Sample Question Papers Practiced\", \"Extracurricular Activities\"]\n","X = pd.DataFrame(df[feature_vars])\n","y = pd.Series(df[\"Performance Index\"]) # Target: Performance Index\n","\n","# Display a preview of the dataset\n","print(\"Dataset preview:\")\n","print(X.head())\n","print(\"\\nTarget variable preview:\")\n","print(y.head())"],"metadata":{"id":"EsIxmUI-Wo0O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 1: Linear Regression on Unscaled Data\n","In this section, we build a [linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) model on the raw data.\n","This helps us see the effect of differing scales on the coefficients.\n","We start by [spliting our data into training and testing sets](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split)."],"metadata":{"id":"bVk3NAYM5wbe"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","\n","# Split the raw data (80% training, 20% testing)\n","X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize and train the linear regression model on unscaled data\n","lin_reg_raw = LinearRegression()\n","lin_reg_raw.fit(X_train_raw, y_train)\n","\n","# Make predictions on the test set\n","y_pred_raw = lin_reg_raw.predict(X_test_raw)"],"metadata":{"id":"xV6-TqL951SG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import mean_squared_error, root_mean_squared_error, r2_score\n","\n","# Evaluate model performance\n","mse_raw = mean_squared_error(y_test, y_pred_raw)\n","rmse_raw = root_mean_squared_error(y_test, y_pred_raw)\n","\n","r2_raw = r2_score(y_test, y_pred_raw)\n","\n","print(\"Unscaled Data Model:\")\n","print(f\"Mean Squared Error: {mse_raw:.2f}\")\n","print(f\"Root Squared Error: {rmse_raw:.2f}\")\n","print(f\"R² Score: {r2_raw:.2f}\")"],"metadata":{"id":"sUb4UOJa6ALE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Notes on Unscaled Model:\n","- **Coefficients (Unscaled):**\n","    - Each coefficient represents the change in the Performance Index for a one-unit change in the respective feature, holding all other features constant.\n","    - For example, if \"Hours Studied\" has a coefficient of 2.85, it implies that for each additional hour studied, the Performance Index increases by 2.85 points (assuming other factors remain constant).\n","    - However, because features are in different units (e.g., hours vs. scores), comparing these coefficients directly may be misleading.\n","\n","- **R² Score:**\n","    - This metric indicates the proportion of the variance in the target variable explained by the model.\n","    - An R² close to 1 suggests a very good fit, while an R² near 0 indicates the model fails to capture much variance.\n","\n","- **MSE & RMSE:**\n","    - MSE measures the average squared difference between actual and predicted values.\n","    - RMSE, being the square root of MSE, gives an error metric in the same units as the target.\n","    - Lower RMSE values indicate better predictive performance."],"metadata":{"id":"cUldxWi0BKGl"}},{"cell_type":"code","source":["# View our model's coefficients\n","print(\"Model Coefficients (Unscaled):\")\n","print(pd.Series(lin_reg_raw.coef_,\n","                index=X.columns))\n","print(\"\\nModel Intercept (Unscaled):\")\n","print(pd.Series(lin_reg_raw.intercept_))"],"metadata":{"id":"ZB2zO71yBHyG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Manually Computing a Prediction from Our Model\n","- In this section, we'll calculate a predicted value by hand (i.e., by multiplying the model's coefficients by the original feature values and adding the intercept).\n","- This mirrors exactly what the model does internally.\n","\n","- **Why is this helpful?**\n","   - It reinforces how linear regression makes its predictions using the equation: `prediction = intercept + (coef_1 * x_1) + (coef_2 * x_2) + ...`\n","   - It helps us see the individual impact of each feature on the final prediction.\n","   - It confirms that the manual approach matches the `model.predict()` output."],"metadata":{"id":"cqTyCRrOSxTL"}},{"cell_type":"markdown","source":["#### 1. Extract the coefficients and intercept from our trained model"],"metadata":{"id":"WZ7xL8YDTYGd"}},{"cell_type":"code","source":["coef_series = pd.Series(lin_reg_raw.coef_, index=X.columns)\n","intercept = lin_reg_raw.intercept_\n","\n","print(\"Coefficients (Unscaled):\")\n","print(coef_series)\n","print(\"\\nIntercept:\", intercept)"],"metadata":{"id":"uP4ggD8mRLby"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2. Select a single row of our data (e.g., the second row)\n","- We select only the columns that were used as features in our model.\n","- The row's values represent the actual data for Hours Studied, Previous Scores, etc."],"metadata":{"id":"IENlzHSLTV16"}},{"cell_type":"code","source":["# This row's feature values will be multiplied by our coefficients.\n","row_index = 1  # for demonstration\n","row_features = X.iloc[row_index]  # features only\n","print(\"Feature values (Row\", row_index, \"):\\n\", row_features)"],"metadata":{"id":"PR91MrnMQuE7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3. Compute the manual prediction"],"metadata":{"id":"31cUy0iJTpy5"}},{"cell_type":"code","source":["manual_prediction = (row_features * coef_series).sum() + intercept\n","print(\"\\nManual Prediction for Row\", row_index, \":\", manual_prediction)"],"metadata":{"id":"GHIjwmBvSTNz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explanation:**\n","- We multiply each feature value by its corresponding coefficient and sum them up.\n","- Then, we add the intercept.\n","- This is precisely the linear regression equation:\n","$$\n","\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n","$$\n","\n","Where:\n"," - $\\beta_0$ is the intercept\n"," - $\\beta_i$ is the coefficient for feature $x_i$\n","\n"," Thus, `manual_prediction` should match what the model would predict internally."],"metadata":{"id":"3EMoNVUxTwSZ"}},{"cell_type":"markdown","source":["#### 4. Compare to `model.predict()` for confirmation"],"metadata":{"id":"LcSqpjAUUygV"}},{"cell_type":"code","source":["model_prediction = lin_reg_raw.predict([row_features])\n","print(\"Model Prediction from lin_reg_raw.predict():\", model_prediction[0])"],"metadata":{"id":"1x95-XjDU0U4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Observation:**\n","- The `manual_prediction` and `model_prediction` should be nearly identical (up to minor floating-point differences).\n","- If they match, we've confirmed our understanding of how the model uses coefficients and intercept to make a prediction.\n","\n","### Why This Matters\n","- **Transparency:** It shows exactly how each feature influences the final predicted value.\n","- **Verification:** Confirms our \"manual\" math aligns with the model's internal computation.\n","- **Interpretability:** By inspecting the coefficients, we see which features have the biggest impact (positive or negative) on the Performance Index, and we can discuss whether the magnitudes make sense given the domain context."],"metadata":{"id":"c3SQz1XiVQti"}},{"cell_type":"markdown","source":["## Part 2: Linear Regression on Scaled Data\n","Now we apply feature scaling using StandardScaler and rebuild the model.\n","Scaling brings all features to a similar scale, which aids in the interpretation of the coefficients."],"metadata":{"id":"0u3A7-UF6Fa6"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","# Initialize the scaler and apply it to the features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n","\n","# Split the scaled data\n","X_train_scaled, X_test_scaled, _, _ = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# Initialize and train the linear regression model on scaled data\n","lin_reg_scaled = LinearRegression()\n","lin_reg_scaled.fit(X_train_scaled, y_train)\n","\n","# Make predictions on the test set\n","y_pred_scaled = lin_reg_scaled.predict(X_test_scaled)\n","\n","# Evaluate model performance\n","mse_scaled = mean_squared_error(y_test, y_pred_scaled)\n","r2_scaled = r2_score(y_test, y_pred_scaled)\n","rmse_scaled = root_mean_squared_error(y_test, y_pred_raw)\n","\n","print(\"\\nScaled Data Model:\")\n","print(f\"Mean Squared Error: {mse_scaled:.2f}\")\n","print(f\"Root Mean Squared Error: {rmse_scaled:.2f}\")\n","print(f\"R² Score: {r2_scaled:.2f}\")\n","print(\"Model Coefficients (Scaled):\")\n","print(pd.Series(lin_reg_scaled.coef_, index=X.columns))"],"metadata":{"id":"u891WVe05rk0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Notes on Scaled Model:\n","- **Coefficients (Scaled):**\n","    - After scaling, each coefficient indicates the change in the Performance Index for a one standard deviation change in that feature.\n","    - This standardization makes it easier to compare the relative importance of features.\n","    - For example, a higher coefficient means that feature has a larger effect on the target, per standard deviation change.\n","\n","- **R² and RMSE Comparison:**\n","    - Often the overall performance metrics (R² and RMSE) do not change dramatically after scaling for linear regression.\n","    - However, scaling is essential for interpreting the model coefficients correctly, especially when features are on different scales.\n","    - It is also a critical preprocessing step for many other algorithms."],"metadata":{"id":"u-_4BofGByvU"}},{"cell_type":"markdown","source":["# Conclusion\n","In this demo, we:\n","- Built and evaluated a linear regression model on unscaled data.\n","- Re-trained the model after applying feature scaling.\n","- Observed that while overall performance metrics (**MSE** and **R²**) may be similar, scaling is crucial for the interpretability of model coefficients and for ensuring that features contribute in a balanced way.\n","  \n","### Key Takeaways:\n","- **Coefficients:** On unscaled data, coefficients are tied to the original units, which can be hard to compare.\n","  After scaling, coefficients represent the effect of a one standard deviation change in the feature.\n","- **R² Score:** Reflects the proportion of variance in the target variable explained by the model.\n","- **MSE (and RMSE):** Lower values indicate better model performance; RMSE provides an error measure in the target's units.\n","\n","This process reflects the \"remember-formulate-predict\" approach in machine learning."],"metadata":{"id":"TSFowtR16PYv"}}]}